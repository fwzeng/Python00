{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20.2\n"
     ]
    }
   ],
   "source": [
    "# update sklearn\n",
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error for dropping columns with missing values:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fwzen\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187115.0\n",
      "Mean absolute error for filling missing values using imputation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fwzen\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182532.0\n",
      "Mean absolute error for filling missing values using imputation and recording what was imputed:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fwzen\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184363.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# load data\n",
    "melb_data = pd.read_csv('melb_data.csv')\n",
    "\n",
    "melb_target = melb_data.Price\n",
    "melb_predictors = melb_data.drop(['Price'], axis=1)\n",
    "\n",
    "# use only numeric predictors for now\n",
    "melb_numeric_predictors = melb_predictors.select_dtypes(exclude='object')\n",
    "\n",
    "# split data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(melb_numeric_predictors, \n",
    "                                                    melb_target,\n",
    "                                                    train_size=0.7,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# define a function to check mae\n",
    "def score_dataset(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    predicts = model.predict(X_test)\n",
    "    return mean_absolute_error(predicts, y_test)\n",
    "\n",
    "# case 1: drop missing values\n",
    "cols_with_missing = [col for col in X_train.columns\n",
    "                         if X_train[col].isnull().any()]\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "reduced_X_test = X_test.drop(cols_with_missing, axis=1)\n",
    "print('Mean absolute error for dropping columns with missing values:')\n",
    "print(round(score_dataset(reduced_X_train, reduced_X_test, y_train, y_test)))\n",
    "\n",
    "# case 2: fill missing values using imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train = my_imputer.fit_transform(X_train)\n",
    "imputed_X_test = my_imputer.transform(X_test)\n",
    "print('Mean absolute error for filling missing values using imputation:')\n",
    "print(round(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test)))\n",
    "\n",
    "# case 3: fill missing values using imputation and record what was imputed\n",
    "imputed_X_train_plus = X_train.copy()\n",
    "imputed_X_test_plus = X_test.copy()\n",
    "\n",
    "cols_with_missing = [col for col in X_train.columns\n",
    "                         if X_train[col].isnull().any()]\n",
    "\n",
    "for col in cols_with_missing:\n",
    "    imputed_X_train_plus[col + 'was_missing'] = imputed_X_train_plus[col].isnull()\n",
    "    imputed_X_test_plus[col + 'was_missing'] = imputed_X_test_plus[col].isnull()\n",
    "\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train_plus = my_imputer.fit_transform(imputed_X_train_plus)\n",
    "imputed_X_test_plus = my_imputer.transform(imputed_X_test_plus)\n",
    "print('Mean absolute error for filling missing values using imputation and recording what was imputed:')\n",
    "print(round(score_dataset(imputed_X_train_plus, imputed_X_test_plus, y_train, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function cross_val_score in module sklearn.model_selection._validation:\n",
      "\n",
      "cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv='warn', n_jobs=None, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', error_score='raise-deprecating')\n",
      "    Evaluate a score by cross-validation\n",
      "    \n",
      "    Read more in the :ref:`User Guide <cross_validation>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    estimator : estimator object implementing 'fit'\n",
      "        The object to use to fit the data.\n",
      "    \n",
      "    X : array-like\n",
      "        The data to fit. Can be for example a list, or an array.\n",
      "    \n",
      "    y : array-like, optional, default: None\n",
      "        The target variable to try to predict in the case of\n",
      "        supervised learning.\n",
      "    \n",
      "    groups : array-like, with shape (n_samples,), optional\n",
      "        Group labels for the samples used while splitting the dataset into\n",
      "        train/test set.\n",
      "    \n",
      "    scoring : string, callable or None, optional, default: None\n",
      "        A string (see model evaluation documentation) or\n",
      "        a scorer callable object / function with signature\n",
      "        ``scorer(estimator, X, y)``.\n",
      "    \n",
      "    cv : int, cross-validation generator or an iterable, optional\n",
      "        Determines the cross-validation splitting strategy.\n",
      "        Possible inputs for cv are:\n",
      "    \n",
      "        - None, to use the default 3-fold cross validation,\n",
      "        - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
      "        - :term:`CV splitter`,\n",
      "        - An iterable yielding (train, test) splits as arrays of indices.\n",
      "    \n",
      "        For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
      "        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "        other cases, :class:`KFold` is used.\n",
      "    \n",
      "        Refer :ref:`User Guide <cross_validation>` for the various\n",
      "        cross-validation strategies that can be used here.\n",
      "    \n",
      "        .. versionchanged:: 0.20\n",
      "            ``cv`` default value if None will change from 3-fold to 5-fold\n",
      "            in v0.22.\n",
      "    \n",
      "    n_jobs : int or None, optional (default=None)\n",
      "        The number of CPUs to use to do the computation.\n",
      "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "        for more details.\n",
      "    \n",
      "    verbose : integer, optional\n",
      "        The verbosity level.\n",
      "    \n",
      "    fit_params : dict, optional\n",
      "        Parameters to pass to the fit method of the estimator.\n",
      "    \n",
      "    pre_dispatch : int, or string, optional\n",
      "        Controls the number of jobs that get dispatched during parallel\n",
      "        execution. Reducing this number can be useful to avoid an\n",
      "        explosion of memory consumption when more jobs get dispatched\n",
      "        than CPUs can process. This parameter can be:\n",
      "    \n",
      "            - None, in which case all the jobs are immediately\n",
      "              created and spawned. Use this for lightweight and\n",
      "              fast-running jobs, to avoid delays due to on-demand\n",
      "              spawning of the jobs\n",
      "    \n",
      "            - An int, giving the exact number of total jobs that are\n",
      "              spawned\n",
      "    \n",
      "            - A string, giving an expression as a function of n_jobs,\n",
      "              as in '2*n_jobs'\n",
      "    \n",
      "    error_score : 'raise' | 'raise-deprecating' or numeric\n",
      "        Value to assign to the score if an error occurs in estimator fitting.\n",
      "        If set to 'raise', the error is raised.\n",
      "        If set to 'raise-deprecating', a FutureWarning is printed before the\n",
      "        error is raised.\n",
      "        If a numeric value is given, FitFailedWarning is raised. This parameter\n",
      "        does not affect the refit step, which will always raise the error.\n",
      "        Default is 'raise-deprecating' but from version 0.22 it will change\n",
      "        to np.nan.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    scores : array of float, shape=(len(list(cv)),)\n",
      "        Array of scores of the estimator for each run of the cross validation.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn import datasets, linear_model\n",
      "    >>> from sklearn.model_selection import cross_val_score\n",
      "    >>> diabetes = datasets.load_diabetes()\n",
      "    >>> X = diabetes.data[:150]\n",
      "    >>> y = diabetes.target[:150]\n",
      "    >>> lasso = linear_model.Lasso()\n",
      "    >>> print(cross_val_score(lasso, X, y, cv=3))  # doctest: +ELLIPSIS\n",
      "    [0.33150734 0.08022311 0.03531764]\n",
      "    \n",
      "    See Also\n",
      "    ---------\n",
      "    :func:`sklearn.model_selection.cross_validate`:\n",
      "        To run cross-validation on multiple metrics and also to return\n",
      "        train scores, fit times and score times.\n",
      "    \n",
      "    :func:`sklearn.model_selection.cross_val_predict`:\n",
      "        Get predictions from each split of cross-validation for diagnostic\n",
      "        purposes.\n",
      "    \n",
      "    :func:`sklearn.metrics.make_scorer`:\n",
      "        Make a scorer from a performance metric or loss function.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# help(RandomForestRegressor)\n",
    "# help(cross_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fwzen\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\fwzen\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error when Dropping Categoricals: 18441\n",
      "Mean Abslute Error with One-Hot Encoding: 17969\n"
     ]
    }
   ],
   "source": [
    "# class: include categorical data\n",
    "\n",
    "# Read the data\n",
    "import pandas as pd\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Drop houses where the target is missing\n",
    "train_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "\n",
    "target = train_data.SalePrice\n",
    "\n",
    "# Since missing values isn't the focus of this tutorial, we use the simplest\n",
    "# possible approach, which drops these columns. \n",
    "# For more detail (and a better approach) to missing values, see\n",
    "# https://www.kaggle.com/dansbecker/handling-missing-values\n",
    "cols_with_missing = [col for col in train_data.columns \n",
    "                                 if train_data[col].isnull().any()]                                  \n",
    "candidate_train_predictors = train_data.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)\n",
    "candidate_test_predictors = test_data.drop(['Id'] + cols_with_missing, axis=1)\n",
    "\n",
    "# \"cardinality\" means the number of unique values in a column.\n",
    "# We use it as our only way to select categorical columns here. This is convenient, though\n",
    "# a little arbitrary.\n",
    "low_cardinality_cols = [cname for cname in candidate_train_predictors.columns if \n",
    "                                candidate_train_predictors[cname].nunique() < 10 and\n",
    "                                candidate_train_predictors[cname].dtype == \"object\"]\n",
    "numeric_cols = [cname for cname in candidate_train_predictors.columns if \n",
    "                                candidate_train_predictors[cname].dtype in ['int64', 'float64']]\n",
    "my_cols = low_cardinality_cols + numeric_cols\n",
    "train_predictors = candidate_train_predictors[my_cols]\n",
    "test_predictors = candidate_test_predictors[my_cols]\n",
    "\n",
    "# train_predictors.dtypes.sample(10)\n",
    "\n",
    "one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def get_mae(X, y):\n",
    "    # multiple by -1 to make positive MAE score instead of neg value returned as sklearn convention\n",
    "    return -1 * cross_val_score(RandomForestRegressor(50), \n",
    "                                X, y, \n",
    "                                scoring = 'neg_mean_absolute_error').mean()\n",
    "\n",
    "predictors_without_categoricals = train_predictors.select_dtypes(exclude=['object'])\n",
    "\n",
    "mae_without_categoricals = get_mae(predictors_without_categoricals, target)\n",
    "\n",
    "mae_one_hot_encoded = get_mae(one_hot_encoded_training_predictors, target)\n",
    "\n",
    "print('Mean Absolute Error when Dropping Categoricals: ' + str(int(mae_without_categoricals)))\n",
    "print('Mean Abslute Error with One-Hot Encoding: ' + str(int(mae_one_hot_encoded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error for including only imputed numerical data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fwzen\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196034.0\n",
      "Mean absolute error for including both imputed numerical data and one hot encoded categorical data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fwzen\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189420.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise:\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def get_mae(X,y):\n",
    "    return -1 * cross_val_score(RandomForestRegressor(50),\n",
    "                                X, y, \n",
    "                                scoring = 'neg_mean_absolute_error').mean()\n",
    "\n",
    "# load data\n",
    "melb_data = pd.read_csv('melb_data.csv')\n",
    "\n",
    "# Drop houses where the target is missing\n",
    "melb_data.dropna(axis=0, subset=['Price'], inplace=True)\n",
    "\n",
    "target = melb_data.Price\n",
    "predictors = melb_data.drop(['Price'], axis=1)\n",
    "# print(predictors.columns)\n",
    "# print(predictors.dtypes)\n",
    "\n",
    "# Seperate numerical and categorical columns\n",
    "# only use categorical columns that have less than 10 unique values\n",
    "num_cols = [col for col in predictors.columns\n",
    "                if predictors[col].dtype in ['int64','float64']]\n",
    "cat_cols = [col for col in predictors.columns\n",
    "                if predictors[col].nunique() < 10 and\n",
    "                   predictors[col].dtype == 'object']\n",
    "# print(num_cols)\n",
    "# print('-------------------')\n",
    "# print(cat_cols)\n",
    "\n",
    "# Fill numerical missing values with imputation and record what was missing\n",
    "\n",
    "num_data = predictors[num_cols]\n",
    "imputed_num_data = num_data.copy()\n",
    "\n",
    "cols_with_missing = [col for col in num_data.columns\n",
    "                         if num_data[col].isnull().any()]\n",
    "\n",
    "for col in cols_with_missing:\n",
    "    imputed_num_data[col + '_was_missing'] = imputed_num_data[col].isnull()\n",
    "    \n",
    "my_impute = SimpleImputer()\n",
    "imputed_num_data = my_impute.fit_transform(imputed_num_data)\n",
    "\n",
    "# One hot encode categorical data\n",
    "\n",
    "cat_data = predictors[cat_cols]\n",
    "one_hot_encoded_cat_data = pd.get_dummies(cat_data)\n",
    "\n",
    "# Put imputed numerical data and one hot encoded categorical data together\n",
    "improved_predictors = np.concatenate((imputed_num_data, one_hot_encoded_cat_data), axis=1)\n",
    "# print(improved_predictors.shape)\n",
    "\n",
    "print('Mean absolute error for including only imputed numerical data:')\n",
    "print(round(get_mae(imputed_num_data, target)))\n",
    "print('Mean absolute error for including both imputed numerical data and one hot encoded categorical data:')\n",
    "print(round(get_mae(improved_predictors, target)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
